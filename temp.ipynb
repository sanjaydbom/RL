{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 0) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 119\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Compute losses\u001b[39;00m\n\u001b[1;32m    118\u001b[0m values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(episode_values)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m--> 119\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(episode_log_probs)\n\u001b[1;32m    120\u001b[0m entropy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(episode_entropy)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    122\u001b[0m advantages \u001b[38;5;241m=\u001b[39m returns \u001b[38;5;241m-\u001b[39m values\u001b[38;5;241m.\u001b[39mdetach()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 0) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        logits = self.fc4(x)  # Raw logits, no ReLU\n",
    "        return F.softmax(logits, dim=-1)\n",
    "\n",
    "# Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "# Hyperparameters\n",
    "actor_lr = 3e-4  # Lowered slightly for stability\n",
    "critic_lr = 1e-3  # Reduced to balance with actor\n",
    "gamma = 0.99\n",
    "num_epochs = 1500  # Increased training time\n",
    "entropy_beta = 0.01  # Entropy regularization for exploration\n",
    "clip_grad = 0.5  # Gradient clipping\n",
    "\n",
    "# Initialize environment and models\n",
    "env = gym.make('LunarLander-v3', continuous=False)\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "actor_optim = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "critic_optim = optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "\n",
    "# Action selection\n",
    "def get_action(state):\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    action_probs = actor(state)\n",
    "    dist = Categorical(action_probs)\n",
    "    action = dist.sample()\n",
    "    return action.item(), dist.log_prob(action), dist.entropy()\n",
    "\n",
    "# Test function\n",
    "def test(num_tests):\n",
    "    tot_reward = 0\n",
    "    for _ in range(num_tests):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _, _ = get_action(state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            tot_reward += reward\n",
    "            done = terminated or truncated\n",
    "    return tot_reward / num_tests\n",
    "\n",
    "# Training loop\n",
    "rewards = []\n",
    "max_reward = -300\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    episode_log_probs = []\n",
    "    episode_values = []\n",
    "    episode_rewards = []\n",
    "    episode_entropy = []\n",
    "    \n",
    "    # Collect episode data\n",
    "    while not done:\n",
    "        action, log_prob, entropy = get_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        value = critic(state)\n",
    "        \n",
    "        episode_log_probs.append(log_prob)\n",
    "        episode_values.append(value)\n",
    "        episode_rewards.append(reward)\n",
    "        episode_entropy.append(entropy)\n",
    "        \n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    # Compute returns and advantages\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in episode_rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    \n",
    "    # Normalize returns\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "    \n",
    "    # Compute losses\n",
    "    values = torch.cat(episode_values).squeeze()\n",
    "    log_probs = torch.cat(episode_log_probs)\n",
    "    entropy = torch.cat(episode_entropy).mean()\n",
    "    \n",
    "    advantages = returns - values.detach()\n",
    "    actor_loss = -(log_probs * advantages.detach()).mean() - entropy_beta * entropy  # Add entropy\n",
    "    critic_loss = advantages.pow(2).mean()\n",
    "    \n",
    "    # Update critic\n",
    "    critic_optim.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    nn.utils.clip_grad_norm_(critic.parameters(), clip_grad)\n",
    "    critic_optim.step()\n",
    "    \n",
    "    # Update actor\n",
    "    actor_optim.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    nn.utils.clip_grad_norm_(actor.parameters(), clip_grad)\n",
    "    actor_optim.step()\n",
    "    \n",
    "    # Track rewards\n",
    "    tot_reward = sum(episode_rewards)\n",
    "    rewards.append(tot_reward)\n",
    "    \n",
    "    # Test and save\n",
    "    if epoch % 50 == 0:\n",
    "        avg = test(10)\n",
    "        if avg > max_reward:\n",
    "            max_reward = avg\n",
    "            torch.save(actor.state_dict(), \"LunarLanderAgent.pt\")\n",
    "        print(f\"{epoch} / {num_epochs} : {avg:.1f}\")\n",
    "\n",
    "# Plot results\n",
    "window = 20\n",
    "avg_rewards = [np.mean(rewards[max(0, i - window):(i + 1)]) for i in range(len(rewards))]\n",
    "plt.plot(avg_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Actor-Critic Lunar Lander\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
