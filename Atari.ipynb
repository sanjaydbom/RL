{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05% done, current score: 1.30, average_score: 1.30, exploration rate: 99.55%\n",
      "0.10% done, current score: 1.40, average_score: 1.35, exploration rate: 99.11%\n",
      "0.15% done, current score: 1.00, average_score: 1.23, exploration rate: 98.66%\n",
      "0.20% done, current score: 2.10, average_score: 1.45, exploration rate: 98.21%\n",
      "0.25% done, current score: 1.70, average_score: 1.50, exploration rate: 97.76%\n",
      "0.30% done, current score: 0.80, average_score: 1.38, exploration rate: 97.31%\n",
      "0.35% done, current score: 1.60, average_score: 1.41, exploration rate: 96.87%\n",
      "0.40% done, current score: 1.10, average_score: 1.38, exploration rate: 96.42%\n",
      "0.45% done, current score: 1.50, average_score: 1.39, exploration rate: 95.97%\n",
      "0.50% done, current score: 0.90, average_score: 1.34, exploration rate: 95.53%\n",
      "0.55% done, current score: 1.30, average_score: 1.34, exploration rate: 95.08%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 174\u001b[0m\n\u001b[1;32m    172\u001b[0m experience_buffer\u001b[38;5;241m.\u001b[39mappend((torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32), action, reward, torch\u001b[38;5;241m.\u001b[39mtensor(next_state, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m truncated \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cur_step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m50_000\u001b[39m:\n\u001b[0;32m--> 174\u001b[0m     replay_exp(mini_batch_size)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cur_step \u001b[38;5;241m%\u001b[39m target_network_update_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    178\u001b[0m     target_network\u001b[38;5;241m.\u001b[39mload_state_dict(agent\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "Cell \u001b[0;32mIn[4], line 132\u001b[0m, in \u001b[0;36mreplay_exp\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m    130\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    131\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(agent\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m10.0\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/rmsprop.py:175\u001b[0m, in \u001b[0;36mRMSprop.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    163\u001b[0m     state_steps: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    165\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    166\u001b[0m         group,\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m         state_steps,\n\u001b[1;32m    173\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m     rmsprop(\n\u001b[1;32m    176\u001b[0m         params_with_grad,\n\u001b[1;32m    177\u001b[0m         grads,\n\u001b[1;32m    178\u001b[0m         square_avgs,\n\u001b[1;32m    179\u001b[0m         grad_avgs,\n\u001b[1;32m    180\u001b[0m         momentum_buffer_list,\n\u001b[1;32m    181\u001b[0m         state_steps,\n\u001b[1;32m    182\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    183\u001b[0m         alpha\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    184\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    185\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    186\u001b[0m         momentum\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    187\u001b[0m         centered\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcentered\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    188\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    189\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    190\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    191\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    192\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/rmsprop.py:511\u001b[0m, in \u001b[0;36mrmsprop\u001b[0;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, state_steps, foreach, maximize, differentiable, capturable, has_complex, lr, alpha, eps, weight_decay, momentum, centered)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    509\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_rmsprop\n\u001b[0;32m--> 511\u001b[0m func(\n\u001b[1;32m    512\u001b[0m     params,\n\u001b[1;32m    513\u001b[0m     grads,\n\u001b[1;32m    514\u001b[0m     square_avgs,\n\u001b[1;32m    515\u001b[0m     grad_avgs,\n\u001b[1;32m    516\u001b[0m     momentum_buffer_list,\n\u001b[1;32m    517\u001b[0m     state_steps,\n\u001b[1;32m    518\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    519\u001b[0m     alpha\u001b[38;5;241m=\u001b[39malpha,\n\u001b[1;32m    520\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    521\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    522\u001b[0m     momentum\u001b[38;5;241m=\u001b[39mmomentum,\n\u001b[1;32m    523\u001b[0m     centered\u001b[38;5;241m=\u001b[39mcentered,\n\u001b[1;32m    524\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    525\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    526\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    527\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    528\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/rmsprop.py:317\u001b[0m, in \u001b[0;36m_single_tensor_rmsprop\u001b[0;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, state_steps, lr, alpha, eps, weight_decay, momentum, centered, maximize, differentiable, capturable, has_complex)\u001b[0m\n\u001b[1;32m    315\u001b[0m     avg \u001b[38;5;241m=\u001b[39m square_avg\u001b[38;5;241m.\u001b[39maddcmul(grad_avg, grad_avg, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqrt_()\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m     avg \u001b[38;5;241m=\u001b[39m square_avg\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n\u001b[1;32m    320\u001b[0m     avg \u001b[38;5;241m=\u001b[39m avg\u001b[38;5;241m.\u001b[39madd(eps)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Agent(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Agent, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "\n",
    "        #NoisyConv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "\n",
    "        nn.Conv2d(4,32,8,4),\n",
    "\n",
    "        nn.ReLU(),\n",
    "\n",
    "        #NoisyConv2d(32, 64, kernel_size=4, stride=2),\n",
    "\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "\n",
    "        nn.ReLU(),\n",
    "\n",
    "        #NoisyConv2d(64, 64, kernel_size=3, stride=1),\n",
    "\n",
    "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "\n",
    "        nn.ReLU()\n",
    "\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out((4,84,84))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "\n",
    "        #NoisyLinear(conv_out_size, 512),\n",
    "\n",
    "        nn.Linear(conv_out_size, 512),\n",
    "\n",
    "        nn.ReLU()\n",
    "\n",
    "        )\n",
    "\n",
    "        self.state_layer = nn.Linear(512,1)#NoisyLinear(512,num_bins)\n",
    "\n",
    "        self.advantage_layer = nn.Linear(512,4)#NoisyLinear(512,num_actions * num_bins)\n",
    "\n",
    "\n",
    "    def _get_conv_out(self, shape: tuple) -> int:\n",
    "\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "\n",
    "        return int(torch.prod(torch.tensor(o.size())))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        x = x / 255.0\n",
    "\n",
    "        x = self.conv(x)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        value = self.state_layer(x)\n",
    "\n",
    "        advantage = self.advantage_layer(x)\n",
    "\n",
    "\n",
    "        value = value.expand_as(advantage)\n",
    "\n",
    "        mean_advantage = advantage.mean(dim=1, keepdim = True)\n",
    "\n",
    "\n",
    "        state_action = value + advantage - mean_advantage\n",
    "\n",
    "\n",
    "        return state_action \n",
    "\n",
    "device = \"mps\"\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.1\n",
    "gamma = 0.99\n",
    "lr = 2.5e-4\n",
    "mini_batch_size = 32\n",
    "max_steps = 10_000_000\n",
    "cur_step = 0\n",
    "target_network_update_frequency = 10_000\n",
    "action_space = [0,1,2,3]\n",
    "experience_buffer = deque(maxlen = 300_000)\n",
    "\n",
    "agent = Agent().to(device=device)\n",
    "target_network = Agent().to(device = device)\n",
    "target_network.load_state_dict(agent.state_dict())\n",
    "optim = torch.optim.RMSprop(agent.parameters(), lr, alpha = 0.95, eps = 0.01) \n",
    "loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "env = gym.make(\"ALE/Breakout-v5\", frameskip = 1)\n",
    "env = AtariPreprocessing(env, frame_skip=4, grayscale_obs=True, scale_obs=False, screen_size=84)\n",
    "env = FrameStackObservation(env, stack_size = 4)\n",
    "\n",
    "best_loss = -10\n",
    "loss_over_time = []\n",
    "\n",
    "model_state, _ = env.reset()\n",
    "model_state = torch.tensor(model_state).to(device)\n",
    "\n",
    "def replay_exp(size = mini_batch_size):\n",
    "    memories = random.sample(experience_buffer, k = size)\n",
    "    state, action, reward, next_state = zip(*memories)\n",
    "\n",
    "    states = torch.stack([s.to(device) for s in state])\n",
    "    actions = torch.tensor(action, dtype = torch.long, device = device)\n",
    "    rewards = torch.tensor(reward, dtype = torch.float32, device = device)\n",
    "    non_terminal_states = [ns is not None for ns in next_state]\n",
    "    next_states = torch.stack([ns.to(device) if ns is not None else torch.zeros_like(model_state).to(device) for ns in next_state])\n",
    "\n",
    "    q_values = agent(states).gather(1, actions.unsqueeze(1))\n",
    "    with torch.no_grad():\n",
    "        target = torch.zeros(size, device=device)\n",
    "        next_action = agent(next_states[non_terminal_states]).argmax(1)\n",
    "        target[non_terminal_states] = gamma * target_network(next_states[non_terminal_states]).gather(1, next_action.unsqueeze(1)).squeeze()\n",
    "        target += rewards\n",
    "        target = target.unsqueeze(1)\n",
    "    loss = loss_fn(q_values, target)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(agent.parameters(), 10.0)\n",
    "    optim.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def select_action(state, test = False):\n",
    "    global epsilon\n",
    "    if not test:\n",
    "        epsilon = max(min_epsilon, epsilon - 9e-7)\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(action_space)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            if not isinstance(state, torch.Tensor):\n",
    "                state = torch.tensor(state, dtype = torch.float32).unsqueeze(0)\n",
    "            state = state.to(device=device)\n",
    "            action_values = agent(state)\n",
    "            return torch.argmax(action_values)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_model(num_tests = 50):\n",
    "    total_reward = 0\n",
    "    for i in range(num_tests):\n",
    "        state, _ = env.reset()\n",
    "        action = 1\n",
    "        while True:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = next_state\n",
    "            action = select_action(state, True)\n",
    "    return total_reward / num_tests\n",
    "\n",
    "\n",
    "while cur_step < max_steps:\n",
    "    state, _ = env.reset()\n",
    "    action = 1\n",
    "    while True:\n",
    "        cur_step += 1\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        reward = max(min(reward, 1), -1)\n",
    "        experience_buffer.append((torch.tensor(state, dtype = torch.float32), action, reward, torch.tensor(next_state, dtype = torch.float32) if not terminated and not truncated else None))\n",
    "        if cur_step > 50_000:\n",
    "            replay_exp(mini_batch_size)\n",
    "\n",
    "\n",
    "        if cur_step % target_network_update_frequency == 0:\n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        if cur_step % 5000 == 0:\n",
    "            test_val = test_model(10)\n",
    "            loss_over_time.append(test_val)\n",
    "            if test_val > best_loss:\n",
    "                best_loss = test_val                    \n",
    "                torch.save(agent.state_dict(), \"AtariAgent.pt\")\n",
    "            print(f\"{cur_step/max_steps * 100:.2f}% done, current score: {test_val:.2f}, average_score: {np.mean(loss_over_time[max(0, (cur_step//5000) - 20):cur_step // 5000]):.2f}, exploration rate: {epsilon * 100:.2f}%\")\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "        action = select_action(state)\n",
    "\n",
    "\n",
    "    \n",
    "plt.plot(loss_over_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
